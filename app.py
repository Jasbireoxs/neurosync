import streamlit as st
import json
import os
from pydantic import BaseModel
from typing import List
from gtts import gTTS
import io

# Only needed if HF_TOKEN is set
try:
    from huggingface_hub import InferenceClient
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False

# ============================================================
#  UI CONFIG
# ============================================================
st.set_page_config(
    layout="wide",
    page_title="GuppShupp â€“ Lifelong Friend",
    page_icon="ðŸ’¬"
)

st.markdown("""
<style>
    .stApp { background-color: #050814; color: #FAFAFA; }
    .stButton>button { border-radius: 999px; font-weight: 600; 
                       border: 1px solid #ff4b7a; background: #111827; }
    .stButton>button:hover { border-color: #ff6b94; }
    .memory-card { background: #111827; padding: 20px; border-radius: 16px; 
                   border-left: 5px solid #ff4b7a; 
                   box-shadow: 0 8px 18px rgba(0,0,0,0.45); }
    .stTextInput>div>div>input, 
    .stChatInputContainer textarea { 
        background-color: #050814; 
        color: #FAFAFA; 
        border-radius: 999px; 
    }
</style>
""", unsafe_allow_html=True)

# ============================================================
# 30-MESSAGE CHAT HISTORY (ASSIGNMENT DATA)
# ============================================================
CHAT_HISTORY_30 = [
    "I'm just setting up my dev environment.", "I hate cluttered UIs, keep it minimal.",
    "My dog Barnaby kept me up all night.", "I need a Python script for ETL.",
    "I prefer snake_case variables.", "I'm feeling anxious about the Q4 deadline.",
    "No long explanations, just code.", "I'm in Seattle (PST).",
    "Watched Dune again, love sci-fi.", "Anxiety is better today after a run.",
    "Refactoring this React component is a pain.", "I'm a Senior Dev, skip the basics.",
    "Cutting down on caffeine.", "How do I handle async errors in Node?",
    "Don't talk to me like a robot.", "Project 'TitanAPI' is launching soon.",
    "Proud of the progress we made.", "Remind me to call mom on Sunday.",
    "I hate dark mode in documentation.", "Impostor syndrome is hitting hard.",
    "Let's discuss the DB schema.", "I use PostgreSQL exclusively.",
    "Functional programming is the way.", "Overwhelmed by Jira tickets.",
    "Must work on Linux.", "Excited for the hackathon.",
    "No emojis in comments.", "Goal: Become a CTO.",
    "Feeling calm and focused.", "Thanks, that was short and sweet."
]

# ============================================================
# MEMORY SCHEMA
# ============================================================
class UserProfile(BaseModel):
    user_preferences: List[str]
    emotional_patterns: str
    facts: List[str]

# ============================================================
# HUGGINGFACE CHAT CLIENT (FIXED)
# ============================================================

@st.cache_resource
def get_hf_client():
    """
    Returns an InferenceClient using a SPECIFIC model to avoid "no recommended model" errors.
    """
    if not HF_AVAILABLE:
        return None
    try:
        # 1. Get token from secrets
        token = st.secrets["HF_TOKEN"] 
    except KeyError:
        return None

    try:
        # 2. FIX: Explicitly use Mistral-7B-Instruct v0.3
        # This prevents the 'Task conversational has no recommended model' error.
        repo_id = "mistralai/Mistral-7B-Instruct-v0.3"
        client = InferenceClient(model=repo_id, token=token)
        return client
    except Exception:
        return None


def hf_chat(messages, max_tokens=400, temperature=0.7):
    """
    Wrapper around HuggingFace InferenceClient.
    """
    client = get_hf_client()
    if client is None:
        return None

    try:
        completion = client.chat.completions.create(
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        text = completion.choices[0].message.content

        # Cleanup <think> tags if model outputs them
        while "<think>" in text and "</think>" in text:
            s = text.find("<think>")
            e = text.find("</think>") + len("</think>")
            text = text[:s] + text[e:]
        return text.strip()
    except Exception as e:
        # st.error(f"âš ï¸ HuggingFace error: {e}") # Uncomment to see raw error
        return None

# ============================================================
# COGNITIVE ENGINE (RULE-BASED MEMORY)
# ============================================================
class CognitiveEngine:
    """
    Deterministic memory extraction from the fixed 30-message history.
    """

    def __init__(self):
        self.memory_file = "long_term_memory.json"

    def extract_profile(self, history: List[str]) -> UserProfile:
        prefs = set()
        facts = set()
        emotion_flags = []

        for msg in history:
            lower = msg.lower()

            # Preferences
            if "hate cluttered uis" in lower or "keep it minimal" in lower:
                prefs.add("Prefers minimal, uncluttered UIs")
            if "prefer snake_case" in lower:
                prefs.add("Prefers snake_case for variables")
            if "no long explanations" in lower:
                prefs.add("Wants concise answers (no long explanations)")
            if "skip the basics" in lower:
                prefs.add("Senior-level explanations only (skip basics)")
            if "no emojis in comments" in lower:
                prefs.add("No emojis in code comments")
            if "hate dark mode" in lower:
                prefs.add("Dislikes dark mode in documentation")
            if "postgresql" in lower:
                prefs.add("Prefers PostgreSQL as primary database")
            if "must work on linux" in lower:
                prefs.add("Wants solutions compatible with Linux")
            if "functional programming" in lower:
                prefs.add("Favors functional programming style")

            # Emotional patterns
            if "anxious" in lower:
                emotion_flags.append("anxiety around deadlines and work")
            if "impostor syndrome" in lower:
                emotion_flags.append("impostor syndrome")
            if "overwhelmed" in lower:
                emotion_flags.append("overwhelmed by task load (Jira tickets)")
            if "excited" in lower:
                emotion_flags.append("excitement about projects and hackathons")
            if "calm and focused" in lower:
                emotion_flags.append("ability to reach a calm, focused state")

            # Facts
            if "my dog" in lower and "barnaby" in lower:
                facts.add("Has a dog named Barnaby")
            if "i'm in seattle" in lower or "seattle (pst)" in lower:
                facts.add("Lives in Seattle (PST)")
            if "senior dev" in lower:
                facts.add("Is a Senior Developer")
            if "etl" in lower:
                facts.add("Works with Python ETL scripts")
            if "project 'titanapi'" in lower:
                facts.add("Working on a project called 'TitanAPI'")
            if "remind me to call mom on sunday" in lower:
                facts.add("Values family (reminder to call mom on Sundays)")
            if "goal: become a cto" in lower:
                facts.add("Long-term goal: Become a CTO")
            if "dune" in lower and "sci-fi" in lower:
                facts.add("Enjoys sci-fi, especially the movie Dune")
            if "cutting down on caffeine" in lower:
                facts.add("Trying to cut down on caffeine")

        if not emotion_flags:
            emotional_patterns = "Emotionally stable; no strong recurring negative patterns detected."
        else:
            emotional_patterns = (
                "Frequently experiences " 
                + ", ".join(sorted(set(emotion_flags))) 
                + ", but remains motivated and proud of progress."
            )

        return UserProfile(
            user_preferences=sorted(prefs),
            emotional_patterns=emotional_patterns,
            facts=sorted(facts),
        )

    def save(self, profile: UserProfile):
        with open(self.memory_file, "w") as f:
            f.write(profile.model_dump_json(indent=2))

    def load(self):
        if os.path.exists(self.memory_file):
            with open(self.memory_file, "r") as f:
                return UserProfile(**json.load(f))
        return None

# ============================================================
# PERSONA ENGINE â€” OFFLINE TEMPLATES
# ============================================================
def persona_rules(persona: str) -> str:
    return {
        "Calm Mentor": "Warm, calm, encouraging. Normalize struggles, give small steps.",
        "Witty Friend": "Casual, playful, a bit cheeky but caring.",
        "Therapist-style Guide": "Reflect emotions, ask gentle questions, supportive.",
        "No-Nonsense CTO": "Direct, blunt about priorities, action-focused.",
        "Neutral": "Plain, factual, minimal style.",
    }[persona]


def generate_reply_offline(user_msg: str, profile: UserProfile, persona: str = "Neutral") -> str:
    """
    Offline fallback persona replies (no LLM).
    """
    msg = user_msg.strip()
    lower = msg.lower()

    emotional_keywords = [
        "lonely", "alone", "sad", "depressed", "down", 
        "anxious", "anxiety", "stressed", "burned out", "burnt out",
        "tired of", "drained", "overwhelmed", "heartbroken"
    ]
    work_keywords = [
        "bug", "deadline", "q4", "ticket", "jira", "deploy", "release",
        "code", "pr", "merge", "etl", "database", "sql", "async",
        "error", "react", "schema", "node", "python"
    ]

    is_emotional = any(k in lower for k in emotional_keywords)
    is_work = any(k in lower for k in work_keywords)

    work_tail = ""
    if is_work:
        if any("PostgreSQL" in p for p in profile.user_preferences):
            work_tail += " Since you like PostgreSQL, keep your data design tight."
        if any("Linux" in p for p in profile.user_preferences):
            work_tail += " Also, remember everything should behave well on Linux."

    support_tail = ""
    if "Has a dog named Barnaby" in profile.facts:
        support_tail += " Maybe spending a few minutes with Barnaby could help you feel grounded."
    
    # EMOTIONAL PATH
    if is_emotional and not is_work:
        if persona == "Calm Mentor":
            return (
                f"GuppShupp hears you: \"{msg}\".\n\n"
                "Loneliness or stress can feel huge, but it doesnâ€™t define you.\n"
                "Letâ€™s keep things small: take a breath, maybe step away for 5 minutes.\n"
                f"{support_tail}"
            )
        
        if persona == "Witty Friend":
            return (
                f"Real talk: \"{msg}\".\n\n"
                "Your brain is being a drama queen. You're doing fine.\n"
                "Go grab a snack, pet Barnaby, and reset.\n"
            )

        if persona == "No-Nonsense CTO":
            return (
                f"Input: \"{msg}\".\n\n"
                "Burnout kills productivity. If you're overwhelmed, cut scope immediately.\n"
                "Take a break. That's an order."
            )
        
        return f"I hear you saying: \"{msg}\". Take it easy on yourself today."

    # WORK PATH (Default)
    if persona == "Calm Mentor":
        return f"Let's break \"{msg}\" down into small steps. You've got this. {work_tail}"

    if persona == "Witty Friend":
        return f"Ugh, \"{msg}\"? Classic dev life. Fix it and ship it! {work_tail}"

    if persona == "No-Nonsense CTO":
        return f"Regarding \"{msg}\": Prioritize the blocker, ignore the noise. Ship it. {work_tail}"

    return f"[{persona}] {msg} {work_tail}"

# ============================================================
# PERSONA ENGINE â€” HUGGINGFACE VERSION
# ============================================================
def generate_reply_hf(user_msg: str, profile: UserProfile, persona: str = "Neutral") -> str | None:
    """
    Uses HuggingFace LLM (via InferenceClient) to generate replies.
    """
    client = get_hf_client()
    if client is None:
        return None

    system_prompt = f"""
You are GuppShupp, a lifelong AI friend.

PERSONA: {persona}
PERSONA_TRAITS: {persona_rules(persona)}

USER MEMORY:
- Preferences: {profile.user_preferences}
- Emotional patterns: {profile.emotional_patterns}
- Facts: {profile.facts}

GOAL:
- Respond like a caring, emotionally intelligent friend.
- If the user talks about work/tech, be practical and focused.
- Do NOT mention that you are using 'memory' or 'profile'; just speak naturally.
- Keep answers concise (1â€“3 short paragraphs).
- No emojis unless the user uses them first.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_msg},
    ]

    text = hf_chat(messages, max_tokens=350, temperature=0.7)
    return text

# ============================================================
# TEXT-TO-SPEECH
# ============================================================
def tts(text: str):
    try:
        obj = gTTS(text=text, lang="en")
        fp = io.BytesIO()
        obj.write_to_fp(fp)
        fp.seek(0)
        return fp
    except Exception:
        return None

# ============================================================
# MAIN APP
# ============================================================
def main():
    # Header with logo + brand
    header_col1, header_col2 = st.columns([1, 4])

    with header_col1:
        st.markdown("ðŸ’¬")

    with header_col2:
        st.markdown(
            "<h1 style='margin-bottom:0;'>GuppShupp â€“ Lifelong Friend</h1>", 
            unsafe_allow_html=True,
        )
        mode_text = "LLM-powered (HuggingFace)" if get_hf_client() else "Offline fallback mode (No HF Token)"
        st.markdown(
            f"<p style='color:#d1d5db;'>{mode_text}</p>", 
            unsafe_allow_html=True,
        )

    brain = CognitiveEngine()

    # -------------------- 1. MEMORY EXTRACTION --------------------
    st.subheader("1. How GuppShupp Remembers You")
    
    col1, col2 = st.columns(2)
    
    with col1:
        with st.expander("ðŸ“„ See your 30-message story"):
            st.code(CHAT_HISTORY_30)
        
        if st.button("ðŸ§  Build Memory Profile", use_container_width=True):
            with st.spinner("GuppShupp is quietly connecting the dots..."):
                profile = brain.extract_profile(CHAT_HISTORY_30)
                brain.save(profile)
                st.session_state["profile"] = profile
                st.success("âœ… Memory profile created.")

    with col2:
        if "profile" not in st.session_state:
            loaded = brain.load()
            if loaded:
                st.session_state["profile"] = loaded
        
        if "profile" in st.session_state:
            p = st.session_state["profile"]
            st.markdown(f"""
            <div class="memory-card">
                <h4>ðŸ‘¤ What GuppShupp Learns About You</h4>
                <p><b>Preferences:</b><br>{", ".join(p.user_preferences) or "Not enough data yet."}</p>
                <p><b>Emotional Patterns:</b><br>{p.emotional_patterns}</p>
                <p><b>Facts Worth Remembering:</b><br>{", ".join(p.facts) or "None extracted yet."}</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.info("Click **Build Memory Profile** to let GuppShupp learn.")

    st.divider()

    # -------------------- 2. PERSONA ENGINE --------------------
    st.subheader("2. GuppShuppâ€™s Personality Modes")

    persona = st.selectbox(
        "How should GuppShupp talk right now?",
        ["Calm Mentor", "Witty Friend", "Therapist-style Guide", "No-Nonsense CTO"]
    )

    user_msg = st.chat_input("Tell GuppShupp whatâ€™s on your mind...")

    if user_msg and "profile" in st.session_state:
        profile = st.session_state["profile"]
        
        # 1. Try HF generation first
        neutral_hf = generate_reply_hf(user_msg, profile, persona="Neutral")
        # 2. Fallback to offline if HF fails (returns None)
        neutral = neutral_hf if neutral_hf else generate_reply_offline(user_msg, profile, persona="Neutral")

        persona_hf = generate_reply_hf(user_msg, profile, persona=persona)
        styled = persona_hf if persona_hf else generate_reply_offline(user_msg, profile, persona=persona)

        left, right = st.columns(2)
        
        with left:
            with st.chat_message("assistant"):
                st.markdown("### ðŸ”¹ Neutral GuppShupp")
                st.write(neutral)
        
        with right:
            with st.chat_message("assistant"):
                st.markdown(f"### ðŸ”¸ {persona} GuppShupp")
                st.write(styled)
                
                audio = tts(styled)
                if audio:
                    st.audio(audio, format="audio/mp3")


if __name__ == "__main__":
    main()
